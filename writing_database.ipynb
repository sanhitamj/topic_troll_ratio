{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import urllib2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To write down all the urls to be scraped for articles and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links stored is  29\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "# Which links to look for.\n",
    "\n",
    "# years = [\"2015\", \"2016\", \"2017\"]\n",
    "# months = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "# dates = [str(n) for n in xrange(1,28)]\n",
    "\n",
    "years = [\"2017\"]\n",
    "months = [\"oct\"]\n",
    "dates = [str(31)]\n",
    "\n",
    "\n",
    "# count of number of links\n",
    "i = 0\n",
    "\n",
    "# Initiating mongoDB. Using pymongo to connect the database\n",
    "client = MongoClient()\n",
    "db = client[\"guardian\"] #This is the name of the database\n",
    "urls = db[\"urls\"] # this is the table in that database\n",
    "\n",
    "result = db.urls.delete_many({}) # A fresh start to the DB table -> removing all entries \n",
    "all_links = []\n",
    "\n",
    "for month in months:\n",
    "    for date in dates:\n",
    "        root_url = \"https://www.theguardian.com/commentisfree/2016/\" + month + \"/\" + date \n",
    "        \n",
    "        # This one contains all the URLs. Soup extracts those.\n",
    "        list_of_urls = requests.get(root_url).text\n",
    "        soup = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "        # the .findAll method from re finds the _content_ that has \n",
    "        # html tag - 'a', attribute - 'href' and \n",
    "        # the pattern that begins with the root_url variable\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(root_url)}):\n",
    "            all_links.append(link.get('href'))\n",
    "            \n",
    "all_links = set(all_links)\n",
    "for n, link in enumerate(all_links):\n",
    "    # urls is the name of the mongo DataBase. insert_one is the method \n",
    "    if n < 2:\n",
    "        urls.insert_one({\"id\" : i, \"url\" : link})\n",
    "#             print link\n",
    "    i += 1\n",
    "print \"Total number of links stored is \", i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To download comments from guardian website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_comments(guardianLink):\n",
    "    '''\n",
    "    This function downloads the comments, along with title, topic of the original thread\n",
    "    comment_id, no_upvotes, no_replies, comment_author, comment_author_id, etc\n",
    "    Returns BeautifulSoup; need more extraction from the soup for the above-mentioned \n",
    "    parameters\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    #Figuring out the PageId\n",
    "    pageResponse = urllib2.urlopen(guardianLink)\n",
    "    commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "    # print \"Comments Match = \", commentsMatch()\n",
    "    if commentsMatch:\n",
    "        pageId = commentsMatch.group(1)\n",
    "        print '[+] pageId has been retrieved ('+pageId+')'\n",
    "    else:\n",
    "        sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "    #Retrieving comments\n",
    "    downloadCount = 1\n",
    "    downloadError = 0\n",
    "\n",
    "    text = ''\n",
    "\n",
    "    while downloadError == 0:\n",
    "        try:\n",
    "            response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId \n",
    "                                       + '?page=' + str(downloadCount))\n",
    "            html = response.read()\n",
    "            length = len(text)\n",
    "            text += html\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "        except:\n",
    "            downloadError = 1\n",
    "\n",
    "    return BeautifulSoup(text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To extract data from soup\n",
    "\n",
    "#### Returns a list of lists - comment_od, comment_text, author_id, author_name, number_of_upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getting_comment_data(soup):\n",
    "    '''\n",
    "    This function takes in the soup of comments. Returns a list of list. \n",
    "    Each element in the list is a list of -\n",
    "    comment_id, comment, author_id, author, number_of_upvotes\n",
    "    \n",
    "    '''\n",
    "       \n",
    "    auth_name_lst = []\n",
    "    comm_id_lst = []\n",
    "    auth_id_lst = []\n",
    "    for lis in soup.find_all('li'):\n",
    "        if 'data-comment-author-id' in (lis.attrs) and 'data-comment-id' in (lis.attrs)and 'data-comment-author' in (lis.attrs):\n",
    "            auth_name_lst.append(lis.attrs['data-comment-author'].encode('utf-8').replace(\"  \", \" \"))\n",
    "            auth_id_lst.append(int(lis.attrs['data-comment-author-id']))\n",
    "            comm_id_lst.append(int(lis.attrs['data-comment-id']))\n",
    "\n",
    "    comments_text = soup.findAll(\"div\", { \"class\" : \"d-comment__body\" })\n",
    "    recommends = soup.findAll(\"span\", {\"class\" : \"d-comment__recommend-count--old\"})\n",
    "    users = soup.findAll(\"span\", {\"itemprop\" : \"givenName\"})\n",
    "\n",
    "    comment_data_list = []\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    for comment_text, upvotes, user, auth_name, auth_id, comment_id in zip(comments_text, recommends, users, auth_name_lst, auth_id_lst, comm_id_lst):\n",
    "        i += 1\n",
    "        if 'comment was removed by a moderator ' not in comment_text.text:\n",
    "            j += 1\n",
    "#             This is the count of comments not removed by the moderator\n",
    "                    \n",
    "\n",
    "#             if auth_name.strip() != user.text.encode('utf-8').strip():\n",
    "#                 print \"something is broken for -\"+ auth_name.strip()+ \"-\"+ user.text.encode('utf-8').strip()\n",
    "#             else :\n",
    "            if not upvotes.txt:\n",
    "                upvote = 0\n",
    "            else : \n",
    "                upvote = int(upvotes.txt)\n",
    "            comment_data_list.append([comment_id, comment_text.text, auth_id, auth_name, upvote])\n",
    "            \n",
    "\n",
    "    return comment_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract topic(s) and text of an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_topics(url):\n",
    "    '''\n",
    "    Given a URL, this function returns topics and contents of the article\n",
    "    \n",
    "    '''\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "\n",
    "    topic_list = [topic.attrs['data-link-name'][9:] for topic in soup.findAll(\"a\", {\"class\" : \"submeta__link\"})\n",
    "        if 'data-link-name' in topic.attrs and 'keyword: ' in topic.attrs['data-link-name']]\n",
    "\n",
    "    for tag in soup.find_all('svg'):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('figure'):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('aside'):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('span'):\n",
    "        tag.decompose()\n",
    "    \n",
    "    article = [str(s) for s in soup.find(\"div\", {\"class\" : \"content__article-body from-content-api js-article__body\"})]\n",
    "    \n",
    "  \n",
    "    return topic_list, article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on this link https://www.theguardian.com/commentisfree/2016/oct/31/prevent-save-lives-families-child-terrorism-programme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanhita/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on this link https://www.theguardian.com/commentisfree/2016/oct/31/i-took-a-prescription-pill-to-get-a-lot-of-work-done-quickly-heres-what-happened\n"
     ]
    }
   ],
   "source": [
    "cursor = urls.find({})\n",
    "for document in cursor: \n",
    "    url = str(document['url'])\n",
    "    id_n = document['_id']\n",
    "    print \"Working on this link\", url\n",
    "#     soup = download_comments(url)\n",
    "#     comment_data_list = getting_comment_data(soup)\n",
    "#     n_comments = len(comment_data_list)\n",
    "#     [comment_id, comment_text.text, auth_id, auth_name, upvote]\n",
    "    topics_list, article = article_topics(url)\n",
    "    urls.insert( {'url': url}, {'article' : article, 'topics_list': topics_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('5a04ea92817c913b47c96357'),\n",
      " u'id': 0,\n",
      " u'url': u'https://www.theguardian.com/commentisfree/2016/oct/31/prevent-save-lives-families-child-terrorism-programme'}\n",
      "{u'_id': ObjectId('5a04ea92817c913b47c96358'),\n",
      " u'id': 1,\n",
      " u'url': u'https://www.theguardian.com/commentisfree/2016/oct/31/i-took-a-prescription-pill-to-get-a-lot-of-work-done-quickly-heres-what-happened'}\n",
      "{u'_id': ObjectId('5a04eab3817c913b47c96359'),\n",
      " u'url': u'https://www.theguardian.com/commentisfree/2016/oct/31/prevent-save-lives-families-child-terrorism-programme'}\n",
      "{u'_id': ObjectId('5a04eab3817c913b47c9635a'),\n",
      " u'url': u'https://www.theguardian.com/commentisfree/2016/oct/31/i-took-a-prescription-pill-to-get-a-lot-of-work-done-quickly-heres-what-happened'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "cursor = urls.find({})\n",
    "for document in cursor: \n",
    "    pprint(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "stopTime = time.time()\n",
    "runningTime = (stopTime - startTime)/60;\n",
    "print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
