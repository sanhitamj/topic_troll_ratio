{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import urllib2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To write down all the urls to be scraped for articles and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "# Which links to look for.\n",
    "\n",
    "# years = [\"2015\", \"2016\", \"2017\"]\n",
    "# months = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "# dates = [str(n) for n in xrange(1,28)]\n",
    "\n",
    "years = [\"2017\"]\n",
    "months = [\"oct\"]\n",
    "dates = [str(31)]\n",
    "\n",
    "\n",
    "# count of number of links\n",
    "i = 0\n",
    "\n",
    "# Initiating mongoDB. Using pymongo to connect the database\n",
    "client = MongoClient()\n",
    "db = client[\"guardian\"] #This is the name of the database\n",
    "urls = db[\"urls\"] # this is the table in that database\n",
    "\n",
    "result = db.urls.delete_many({}) # A fresh start to the DB table -> removing all entries \n",
    "all_links = []\n",
    "\n",
    "for month in months:\n",
    "    for date in dates:\n",
    "        root_url = \"https://www.theguardian.com/commentisfree/2016/\" + month + \"/\" + date \n",
    "        \n",
    "        # This one contains all the URLs. Soup extracts those.\n",
    "        list_of_urls = requests.get(root_url).text\n",
    "        soup = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "        # the .findAll method from re finds the _content_ that has \n",
    "        # html tag - 'a', attribute - 'href' and \n",
    "        # the pattern that begins with the root_url variable\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(root_url)}):\n",
    "            all_links.append(link.get('href'))\n",
    "            \n",
    "all_links = set(all_links)\n",
    "for link in all_links:\n",
    "    # urls is the name of the mongo DataBase. insert_one is the method \n",
    "    urls.insert_one({\"id\" : i, \"url\" : link})\n",
    "#             print link\n",
    "    i += 1\n",
    "print \"Total number of links stored is \", i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To download comments from guardian website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_comments(guardianLink):\n",
    "    '''\n",
    "    This function downloads the comments, along with title, topic of the original thread\n",
    "    comment_id, no_upvotes, no_replies, comment_author, comment_author_id, etc\n",
    "    Returns BeautifulSoup; need more extraction from the soup for the above-mentioned \n",
    "    parameters\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    #Figuring out the PageId\n",
    "    pageResponse = urllib2.urlopen(guardianLink)\n",
    "    commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "    # print \"Comments Match = \", commentsMatch()\n",
    "    if commentsMatch:\n",
    "        pageId = commentsMatch.group(1)\n",
    "        print '[+] pageId has been retrieved ('+pageId+')'\n",
    "    else:\n",
    "        sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "    #Retrieving comments\n",
    "    downloadCount = 1\n",
    "    downloadError = 0\n",
    "\n",
    "    text = ''\n",
    "\n",
    "    while downloadError == 0:\n",
    "        try:\n",
    "            response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId \n",
    "                                       + '?page=' + str(downloadCount))\n",
    "            html = response.read()\n",
    "            length = len(text)\n",
    "            text += html\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "        except:\n",
    "            downloadError = 1\n",
    "\n",
    "    return BeautifulSoup(text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To extract data from soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "stopTime = time.time()\n",
    "runningTime = (stopTime - startTime)/60;\n",
    "print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
