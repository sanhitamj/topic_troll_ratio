{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links stored is  248\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "# Which links to look for.\n",
    "\n",
    "years = [\"2015\", \"2016\", \"2017\"]\n",
    "months = [\"jan\"]\n",
    "dates = [str(n) for n in xrange(14,20)]\n",
    "\n",
    "# count of number of links\n",
    "i = 0\n",
    "\n",
    "# Initiating mongoDB. Using pymongo to connect the database\n",
    "client = MongoClient()\n",
    "db = client[\"guardian\"] #This is the name of the database\n",
    "urls = db[\"urls\"] # this is the table in that database\n",
    "\n",
    "result = db.urls.delete_many({}) # A fresh start to the DB table -> removing all entries \n",
    "\n",
    "for month in months:\n",
    "    for date in dates:\n",
    "        root_url = \"https://www.theguardian.com/commentisfree/2016/\" + month + \"/\" + date \n",
    "        \n",
    "        # This one contains all the URLs. Soup extracts those.\n",
    "        list_of_urls = requests.get(root_url).text\n",
    "        soup = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "        # the .findAll method from re finds the _content_ that has \n",
    "        # html tag - 'a', attribute - 'href' and \n",
    "        # the pattern that begins with the root_url variable\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(root_url)}):\n",
    "\n",
    "            # urls is the name of the mongo DataBase. insert_one is the method \n",
    "            urls.insert_one({str(i) : link.get('href')})\n",
    "            i += 1\n",
    "print \"Total number of links stored is \", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for d in urls.find():\n",
    "    i += 1\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/commentisfree/2017/aug/13/america-carpet-bombed-north-korea-remember-that-past\"\n",
    "\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "# print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The one below is a github code for downloading comments. Do not use this; instead use a modified version of this in the cell below this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# This script will download all comments from a Guardian (theguardian.com) article into a single .html file\n",
    "# Use 'File ID' to easily identify your file later (this could also be something like a Citavi identifier)\n",
    "# Ingo Kleiber <ikleiber@googlemail.com>\n",
    "# https://gist.github.com/IngoKl/3ebd968ef3c6da406638\n",
    "\n",
    "print \"\"\"\\\n",
    "   _____                     _ _             _____  _      \n",
    "  / ____|                   | (_)           |  __ \\| |     \n",
    " | |  __ _   _  __ _ _ __ __| |_  __ _ _ __ | |  | | |     \n",
    " | | |_ | | | |/ _` | '__/ _` | |/ _` | '_ \\| |  | | |     \n",
    " | |__| | |_| | (_| | | | (_| | | (_| | | | | |__| | |____ \n",
    "  \\_____|\\__,_|\\__,_|_|  \\__,_|_|\\__,_|_| |_|_____/|______|\n",
    "\"\"\"                                                         \n",
    "\n",
    "print 'Guardian Comment Downloader v.1.1 - Ingo Kleiber (05.01.2015)\\n'\n",
    "guardianLink = raw_input ('> Guadian Link: ')\n",
    "fileId = raw_input('> File ID: ')\n",
    "\n",
    "#Figuring out the PageId\n",
    "pageResponse = urllib2.urlopen(guardianLink)\n",
    "commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "# print \"Comments Match = \", commentsMatch()\n",
    "if commentsMatch:\n",
    "    pageId = commentsMatch.group(1)\n",
    "    print '[+] pageId has been retrieved ('+pageId+')'\n",
    "else:\n",
    "    sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "#Retrieving comments\n",
    "downloadCount = 1\n",
    "downloadError = 0\n",
    "\n",
    "startTime = time.time()\n",
    "fileName = 'GuardianDownload_' + pageId + '_' + str(startTime) + '_' + fileId + '.html'\n",
    "f = open(fileName, 'a')\n",
    "\n",
    "\n",
    "print '[~] Start downloading pages ...'\n",
    "while downloadError == 0:\n",
    "    try:\n",
    "        response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId + '?page=' + str(downloadCount))\n",
    "        html = response.read()\n",
    "        f.write(html)\n",
    "        if downloadCount == 1:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "        print '[+] Page: ' + str(downloadCount) + ' has been downloaded!'\n",
    "        downloadCount = downloadCount + 1\n",
    "        \n",
    "    except:\n",
    "        downloadError = 1\n",
    "\n",
    "stopTime = time.time()\n",
    "runningTime = (stopTime - startTime)/60;\n",
    "print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes' \t\t\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# This script will download all comments from a Guardian (theguardian.com) article into a single .html file\n",
    "# Use 'File ID' to easily identify your file later (this could also be something like a Citavi identifier)\n",
    "# Ingo Kleiber <ikleiber@googlemail.com>\n",
    "# https://gist.github.com/IngoKl/3ebd968ef3c6da406638\n",
    "\n",
    "def download_comments(url, filename):\n",
    "\n",
    "    guardianLink = url\n",
    "    fileId = filename\n",
    "\n",
    "    #Figuring out the PageId\n",
    "    pageResponse = urllib2.urlopen(guardianLink)\n",
    "    commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "    if commentsMatch:\n",
    "        pageId = commentsMatch.group(1)\n",
    "        print '[+] pageId has been retrieved ('+pageId+')'\n",
    "    else:\n",
    "        sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "    #Retrieving comments\n",
    "    downloadCount = 1\n",
    "    downloadError = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    fileName = 'GuardianComments_' + pageId + '_' + str(startTime) + '_' + fileId + '.html'\n",
    "    f = open(fileName, 'a')\n",
    "\n",
    "\n",
    "    print '[~] Start downloading pages ...'\n",
    "    while downloadError == 0:\n",
    "        try:\n",
    "            response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId + '?page=' + str(downloadCount))\n",
    "            html = response.read()\n",
    "#             f.write(html)\n",
    "            if downloadCount == 1:\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "            print '[+] Page: ' + str(downloadCount) + ' has been downloaded!'\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "        except:\n",
    "            downloadError = 1\n",
    "\n",
    "    stopTime = time.time()\n",
    "    runningTime = (stopTime - startTime)/60;\n",
    "    print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes' \t\t\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_comments(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.comment_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = soup.findAll(\"div\", { \"class\" : \"d-comment__body\" })\n",
    "recommends = soup.findAll(\"span\", {\"class\" : \"d-comment__recommend-count--old\"})\n",
    "users = soup.findAll(\"span\", {\"itemprop\" : \"givenName\"})\n",
    "replies = soup.findAll(\"span\", {\"class\" : \"d-comment__reply-to-author\"})\n",
    "for user, comment, recommend, reply in zip(users, comments, recommends, replies):\n",
    "    print user.text, comment.text, recommend.text, reply.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a class=\"social__action social-icon-wrapper\" data-link-name=\"social-comment : facebook\" href=\"https://www.facebook.com/dialog/share?app_id=180444840287&amp;href=https%3A%2F%2Fdiscussion.theguardian.com%2Fcomment-permalink%2F103303986&amp;quote=alexandrerizvin%20commented%3A%20%22My%20fantasy%20of%20freedom%20and%20independence%20has%20been%20to%20have%20no%20dependence%20on%20cars.%22\" target=\"_blank\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "list_of_urls = requests.get(url).text\n",
    "\n",
    "# from bs4 import BeautifulSoup \n",
    "soup2 = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "# soup2.prettify()\n",
    "\n",
    "import re\n",
    "i = 0\n",
    "for link in soup2.findAll('a', attrs={'href': re.compile(\"https://www.theguardian.com/commentisfree/2017\")}):\n",
    "    print link.get('href')\n",
    "    i += 1\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
