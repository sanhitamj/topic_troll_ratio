{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links stored is  536\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "# Which links to look for.\n",
    "\n",
    "years = [\"2015\", \"2016\", \"2017\"]\n",
    "months = [\"jan\", \"feb\"]\n",
    "dates = [str(n) for n in xrange(14,20)]\n",
    "\n",
    "# count of number of links\n",
    "i = 0\n",
    "\n",
    "# Initiating mongoDB. Using pymongo to connect the database\n",
    "client = MongoClient()\n",
    "db = client[\"guardian\"] #This is the name of the database\n",
    "urls = db[\"urls\"] # this is the table in that database\n",
    "\n",
    "result = db.urls.delete_many({}) # A fresh start to the DB table -> removing all entries \n",
    "\n",
    "for month in months:\n",
    "    for date in dates:\n",
    "        root_url = \"https://www.theguardian.com/commentisfree/2016/\" + month + \"/\" + date \n",
    "        \n",
    "        # This one contains all the URLs. Soup extracts those.\n",
    "        list_of_urls = requests.get(root_url).text\n",
    "        soup = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "        # the .findAll method from re finds the _content_ that has \n",
    "        # html tag - 'a', attribute - 'href' and \n",
    "        # the pattern that begins with the root_url variable\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(root_url)}):\n",
    "\n",
    "            # urls is the name of the mongo DataBase. insert_one is the method \n",
    "            urls.insert_one({str(i) : link.get('href')})\n",
    "            i += 1\n",
    "print \"Total number of links stored is \", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links in the database table called url is 536\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for d in urls.find():\n",
    "    i += 1\n",
    "print \"Total number of links in the database table called url is\", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This does the exact same job as the previous one; \n",
    "# but left the previous cell just for the sake of understanding.\n",
    "urls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cursor' object has no attribute 'pretty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-cb279fd18faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cursor' object has no attribute 'pretty'"
     ]
    }
   ],
   "source": [
    "urls.find().pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/commentisfree/2017/aug/13/america-carpet-bombed-north-korea-remember-that-past\"\n",
    "\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "# print soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Do not run the cell below this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _____                     _ _             _____  _      \n",
      "  / ____|                   | (_)           |  __ \\| |     \n",
      " | |  __ _   _  __ _ _ __ __| |_  __ _ _ __ | |  | | |     \n",
      " | | |_ | | | |/ _` | '__/ _` | |/ _` | '_ \\| |  | | |     \n",
      " | |__| | |_| | (_| | | | (_| | | (_| | | | | |__| | |____ \n",
      "  \\_____|\\__,_|\\__,_|_|  \\__,_|_|\\__,_|_| |_|_____/|______|\n",
      "\n",
      "Guardian Comment Downloader v.1.1 - Ingo Kleiber (05.01.2015)\n",
      "\n",
      "> Guadian Link: https://www.theguardian.com/commentisfree/2016/jan/12/rupert-murdoch-jerry-hall-celebrity-humblebrag-times\n",
      "> File ID: try\n",
      "[+] pageId has been retrieved (4fmtq)\n",
      "[~] Start downloading pages ...\n",
      "[+] Page: 1 has been downloaded!\n",
      "[+] Page: 2 has been downloaded!\n",
      "[+] Page: 3 has been downloaded!\n",
      "[+] Page: 4 has been downloaded!\n",
      "[+] Page: 5 has been downloaded!\n",
      "[=] 5 pages have been downloaded in 0.06 minutes\n"
     ]
    }
   ],
   "source": [
    "# The one below is a github code for downloading comments. Do not use this; \n",
    "# instead use a modified version of this in the cell below this one.\n",
    "\n",
    "import urllib2\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# This script will download all comments from a Guardian (theguardian.com) article into a single .html file\n",
    "# Use 'File ID' to easily identify your file later (this could also be something like a Citavi identifier)\n",
    "# Ingo Kleiber <ikleiber@googlemail.com>\n",
    "# https://gist.github.com/IngoKl/3ebd968ef3c6da406638\n",
    "\n",
    "print \"\"\"\\\n",
    "   _____                     _ _             _____  _      \n",
    "  / ____|                   | (_)           |  __ \\| |     \n",
    " | |  __ _   _  __ _ _ __ __| |_  __ _ _ __ | |  | | |     \n",
    " | | |_ | | | |/ _` | '__/ _` | |/ _` | '_ \\| |  | | |     \n",
    " | |__| | |_| | (_| | | | (_| | | (_| | | | | |__| | |____ \n",
    "  \\_____|\\__,_|\\__,_|_|  \\__,_|_|\\__,_|_| |_|_____/|______|\n",
    "\"\"\"                                                         \n",
    "\n",
    "print 'Guardian Comment Downloader v.1.1 - Ingo Kleiber (05.01.2015)\\n'\n",
    "guardianLink = raw_input ('> Guadian Link: ')\n",
    "fileId = raw_input('> File ID: ')\n",
    "\n",
    "#Figuring out the PageId\n",
    "pageResponse = urllib2.urlopen(guardianLink)\n",
    "commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "# print \"Comments Match = \", commentsMatch()\n",
    "if commentsMatch:\n",
    "    pageId = commentsMatch.group(1)\n",
    "    print '[+] pageId has been retrieved ('+pageId+')'\n",
    "else:\n",
    "    sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "#Retrieving comments\n",
    "downloadCount = 1\n",
    "downloadError = 0\n",
    "\n",
    "startTime = time.time()\n",
    "fileName = 'GuardianDownload_' + pageId + '_' + str(startTime) + '_' + fileId + '.html'\n",
    "f = open(fileName, 'a')\n",
    "\n",
    "\n",
    "print '[~] Start downloading pages ...'\n",
    "while downloadError == 0:\n",
    "    try:\n",
    "        response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId + '?page=' + str(downloadCount))\n",
    "        html = response.read()\n",
    "        f.write(html)\n",
    "        if downloadCount == 1:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "        print '[+] Page: ' + str(downloadCount) + ' has been downloaded!'\n",
    "        downloadCount = downloadCount + 1\n",
    "        \n",
    "    except:\n",
    "        downloadError = 1\n",
    "\n",
    "stopTime = time.time()\n",
    "runningTime = (stopTime - startTime)/60;\n",
    "print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes' \t\t\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def download_comments(url, filename):\n",
    "    guardianLink = url\n",
    "    fileId = filename\n",
    "\n",
    "    #Figuring out the PageId\n",
    "    pageResponse = urllib2.urlopen(guardianLink)\n",
    "    commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "    # print \"Comments Match = \", commentsMatch()\n",
    "    if commentsMatch:\n",
    "        pageId = commentsMatch.group(1)\n",
    "        print '[+] pageId has been retrieved ('+pageId+')'\n",
    "    else:\n",
    "        sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "    #Retrieving comments\n",
    "    downloadCount = 1\n",
    "    downloadError = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    fileName = 'GuardianDownload_' + pageId + '_' + str(startTime) + '_' + fileId + '.html'\n",
    "    text = ''\n",
    "    f = open(fileName, 'a')\n",
    "\n",
    "\n",
    "    print '[~] Start downloading pages ...'\n",
    "    while downloadError == 0:\n",
    "        try:\n",
    "            response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId + '?page=' + str(downloadCount))\n",
    "            html = response.read()\n",
    "            length = len(text)\n",
    "            text += html\n",
    "            if len(text) <= length:\n",
    "                print \"Text appending isn't working as expected\"\n",
    "            else :\n",
    "                print \"Text appending may have worked\"\n",
    "            f.write(html)\n",
    "            if downloadCount == 1:\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "            if downloadCount %5 == 0:\n",
    "                print '[+] Page: ' + str(downloadCount) + ' has been downloaded!'\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "        except:\n",
    "            downloadError = 1\n",
    "\n",
    "    stopTime = time.time()\n",
    "    runningTime = (stopTime - startTime)/60;\n",
    "    print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes' \t\t\n",
    "    f.close()\n",
    "    return BeautifulSoup(text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] pageId has been retrieved (4fmtq)\n",
      "[~] Start downloading pages ...\n",
      "Text appending may have worked\n",
      "Text appending may have worked\n",
      "Text appending may have worked\n",
      "Text appending may have worked\n",
      "Text appending may have worked\n",
      "[+] Page: 5 has been downloaded!\n",
      "[=] 5 pages have been downloaded in 0.06 minutes\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.theguardian.com/commentisfree/2016/jan/12/rupert-murdoch-jerry-hall-celebrity-humblebrag-times\"\n",
    "soup = download_comments(url, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# This function downloads all comments from a Guardian (theguardian.com) article into a \n",
    "# single .html file \n",
    "# Use 'File ID' to easily identify your file later (this could also be something \n",
    "# like a Citavi identifier)\n",
    "# Ingo Kleiber <ikleiber@googlemail.com>\n",
    "# https://gist.github.com/IngoKl/3ebd968ef3c6da406638\n",
    "\n",
    "def download_comments(url, filename):\n",
    "\n",
    "    guardianLink = url\n",
    "    fileId = filename\n",
    "\n",
    "    #Figuring out the PageId\n",
    "    pageResponse = urllib2.urlopen(guardianLink)\n",
    "    commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "    if commentsMatch:\n",
    "        pageId = commentsMatch.group(1)\n",
    "        print '[+] pageId has been retrieved ('+pageId+')'\n",
    "    else:\n",
    "        sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "    #Retrieving comments\n",
    "    downloadCount = 1\n",
    "    downloadError = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    fileName = 'GuardianComments_' + pageId + '_' + str(startTime) + '_' + fileId + '.html'\n",
    "    f = open(fileName, 'a')\n",
    "    text = ''\n",
    "\n",
    "\n",
    "    print '[~] Start downloading pages ...'\n",
    "    while downloadError == 0:\n",
    "        try:\n",
    "            response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId + '?page=' + str(downloadCount))\n",
    "            html = response.read()\n",
    "            length = len(text)\n",
    "            text += html\n",
    "            if len(text) <= length:\n",
    "                print \"Text appending isn't working as expected\"\n",
    "            f.write(html)\n",
    "            \n",
    "            if downloadCount == 1:\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                print type(html)\n",
    "            if pageId % 1 == 0:\n",
    "                print '[+] Page: ' + str(downloadCount) + ' has been downloaded!'\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "        except:\n",
    "            downloadError = 1\n",
    "\n",
    "    stopTime = time.time()\n",
    "    runningTime = (stopTime - startTime)/60;\n",
    "    print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes' \n",
    "    f.close() \n",
    "    \n",
    "    return BeautifulSoup(text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] pageId has been retrieved (4fmtq)\n",
      "[~] Start downloading pages ...\n",
      "<type 'str'>\n",
      "[=] 0 pages have been downloaded in 0.02 minutes\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.theguardian.com/commentisfree/2016/jan/12/rupert-murdoch-jerry-hall-celebrity-humblebrag-times\"\n",
    "soup = download_comments(url, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = soup.findAll(\"div\", { \"class\" : \"d-comment__body\" })\n",
    "recommends = soup.findAll(\"span\", {\"class\" : \"d-comment__recommend-count--old\"})\n",
    "users = soup.findAll(\"span\", {\"itemprop\" : \"givenName\"})\n",
    "replies = soup.findAll(\"span\", {\"class\" : \"d-comment__reply-to-author\"})\n",
    "# for user, comment, recommend, reply in zip(users, comments, recommends, replies):\n",
    "#     print user.text, comment.text, recommend.text, reply.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a class=\"social__action social-icon-wrapper\" data-link-name=\"social-comment : facebook\" href=\"https://www.facebook.com/dialog/share?app_id=180444840287&amp;href=https%3A%2F%2Fdiscussion.theguardian.com%2Fcomment-permalink%2F103303986&amp;quote=alexandrerizvin%20commented%3A%20%22My%20fantasy%20of%20freedom%20and%20independence%20has%20been%20to%20have%20no%20dependence%20on%20cars.%22\" target=\"_blank\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 249, 249, 248)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replies), len(users), len(comments), len(recommends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"GuardianDownload_4fmtq_1508968524.96_test.html\", 'r')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 249, 249, 248)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = BeautifulSoup(text, 'html.parser')\n",
    "comments = s2.findAll(\"div\", { \"class\" : \"d-comment__body\" })\n",
    "recommends = s2.findAll(\"span\", {\"class\" : \"d-comment__recommend-count--old\"})\n",
    "users = s2.findAll(\"span\", {\"itemprop\" : \"givenName\"})\n",
    "replies = s2.findAll(\"span\", {\"class\" : \"d-comment__reply-to-author\"})\n",
    "\n",
    "len(replies), len(users), len(comments), len(recommends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.theguardian.com/commentisfree/2017/oct/25/bill-o-reilly-fox-terrible-men-media\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/bill-o-reilly-fox-terrible-men-media\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/stephen-miller-refugee-policy-trump\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/stephen-miller-refugee-policy-trump\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/why-dont-i-enjoy-sex-google-autocomplete-questions\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/why-dont-i-enjoy-sex-google-autocomplete-questions\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/google-alphabet-sidewalk-labs-toronto\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/google-alphabet-sidewalk-labs-toronto\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/eyewitness-mogadishu-somalia-bomb-devastating-al-shabaab-terror\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/eyewitness-mogadishu-somalia-bomb-devastating-al-shabaab-terror\n",
      "https://www.theguardian.com/commentisfree/2017/oct/23/houston-harvey-construction-workers-protect-abuse-undocumented\n",
      "https://www.theguardian.com/commentisfree/2017/oct/23/houston-harvey-construction-workers-protect-abuse-undocumented\n",
      "https://www.theguardian.com/commentisfree/2017/oct/21/harvey-weinstein-liberal-world\n",
      "https://www.theguardian.com/commentisfree/2017/oct/21/harvey-weinstein-liberal-world\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/daughter-stateless-uk-bahrain-torture-human-rights\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/daughter-stateless-uk-bahrain-torture-human-rights\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/in-our-focus-on-the-digital-have-we-lost-our-sense-of-what-being-human-means\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/in-our-focus-on-the-digital-have-we-lost-our-sense-of-what-being-human-means\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/atheism-ta-nehisi-coates-pessimism-race-relations\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/atheism-ta-nehisi-coates-pessimism-race-relations\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/identity-extremists-donald-trump\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/identity-extremists-donald-trump\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/time-to-reinvent-the-federal-reserve\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/time-to-reinvent-the-federal-reserve\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/harvey-weinstein-bill-cosby-allegations\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/harvey-weinstein-bill-cosby-allegations\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/the-guardian-view-on-chinese-politics-an-age-of-ambition\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/the-guardian-view-on-chinese-politics-an-age-of-ambition\n",
      "https://www.theguardian.com/commentisfree/2017/oct/23/the-guardian-view-on-fox-and-oreilly-more-bad-news\n",
      "https://www.theguardian.com/commentisfree/2017/oct/23/the-guardian-view-on-fox-and-oreilly-more-bad-news\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/bill-o-reilly-fox-terrible-men-media\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/bill-o-reilly-fox-terrible-men-media\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/identity-extremists-donald-trump\n",
      "https://www.theguardian.com/commentisfree/2017/oct/22/identity-extremists-donald-trump\n",
      "https://www.theguardian.com/commentisfree/2017/oct/21/harvey-weinstein-liberal-world\n",
      "https://www.theguardian.com/commentisfree/2017/oct/21/harvey-weinstein-liberal-world\n",
      "https://www.theguardian.com/commentisfree/2017/may/31/ice-agents-out-of-control-immigration-arrests\n",
      "https://www.theguardian.com/commentisfree/2017/may/31/ice-agents-out-of-control-immigration-arrests\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/terry-richardson-vogue-conde-nast-international\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/jeff-flake-no-hero-resistance-republican-party\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/why-dont-i-enjoy-sex-google-autocomplete-questions\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/bill-o-reilly-fox-terrible-men-media\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/stephen-miller-refugee-policy-trump\n",
      "https://www.theguardian.com/commentisfree/2017/oct/23/owning-car-thing-of-the-past-cities-utopian-vision\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/decolonise-cambridge-university-row-attack-students-colour-lola-olufemi-curriculums\n",
      "https://www.theguardian.com/commentisfree/2017/oct/25/sold-sex-fund-unpaid-internships-poor-kids-battling-class-gap\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/google-alphabet-sidewalk-labs-toronto\n",
      "https://www.theguardian.com/commentisfree/2017/oct/24/jeff-flake-speech-donald-trump-senate\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "list_of_urls = requests.get(url).text\n",
    "\n",
    "# from bs4 import BeautifulSoup \n",
    "soup2 = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "# soup2.prettify()\n",
    "\n",
    "import re\n",
    "i = 0\n",
    "for link in soup2.findAll('a', attrs={'href': re.compile(\"https://www.theguardian.com/commentisfree/2017\")}):\n",
    "    print link.get('href')\n",
    "    i += 1\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
