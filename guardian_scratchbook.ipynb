{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "import urllib2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links stored is  29\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "# Which links to look for.\n",
    "\n",
    "# years = [\"2015\", \"2016\", \"2017\"]\n",
    "# months = [\"jan\", \"feb\"]\n",
    "# dates = [str(n) for n in xrange(14,20)]\n",
    "\n",
    "years = [\"2017\"]\n",
    "months = [\"oct\"]\n",
    "dates = [str(31)]\n",
    "\n",
    "\n",
    "# count of number of links\n",
    "i = 0\n",
    "\n",
    "# Initiating mongoDB. Using pymongo to connect the database\n",
    "client = MongoClient()\n",
    "db = client[\"guardian\"] #This is the name of the database\n",
    "urls = db[\"urls\"] # this is the table in that database\n",
    "\n",
    "result = db.urls.delete_many({}) # A fresh start to the DB table -> removing all entries \n",
    "all_links = []\n",
    "\n",
    "for month in months:\n",
    "    for date in dates:\n",
    "        root_url = \"https://www.theguardian.com/commentisfree/2016/\" + month + \"/\" + date \n",
    "        \n",
    "        # This one contains all the URLs. Soup extracts those.\n",
    "        list_of_urls = requests.get(root_url).text\n",
    "        soup = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "        # the .findAll method from re finds the _content_ that has \n",
    "        # html tag - 'a', attribute - 'href' and \n",
    "        # the pattern that begins with the root_url variable\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(root_url)}):\n",
    "            all_links.append(link.get('href'))\n",
    "            \n",
    "all_links = set(all_links)\n",
    "for link in all_links:\n",
    "    # urls is the name of the mongo DataBase. insert_one is the method \n",
    "    urls.insert_one({\"id\" : i, \"url\" : link})\n",
    "#             print link\n",
    "    i += 1\n",
    "print \"Total number of links stored is \", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links in the database table called url is 29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 0\n",
    "for d in urls.find():\n",
    "    i += 1\n",
    "print \"Total number of links in the database table called url is\", i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_comments(guardianLink):\n",
    "    '''\n",
    "    This function downloads the comments, along with title, topic of the original thread\n",
    "    comment_id, no_upvotes, no_replies, comment_author, comment_author_id, etc\n",
    "    Returns BeautifulSoup; need more extraction from the soup for the above-mentioned \n",
    "    parameters\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    #Figuring out the PageId\n",
    "    pageResponse = urllib2.urlopen(guardianLink)\n",
    "    commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "    # print \"Comments Match = \", commentsMatch()\n",
    "    if commentsMatch:\n",
    "        pageId = commentsMatch.group(1)\n",
    "        print '[+] pageId has been retrieved ('+pageId+')'\n",
    "    else:\n",
    "        sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "    #Retrieving comments\n",
    "    downloadCount = 1\n",
    "    downloadError = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    fileName = 'GuardianDownload_' + pageId + '_' + str(startTime) + '_' + fileId + '.html'\n",
    "    text = ''\n",
    "\n",
    "    print '[~] Start downloading pages ...'\n",
    "    while downloadError == 0:\n",
    "        try:\n",
    "            response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId + '?page=' + str(downloadCount))\n",
    "            html = response.read()\n",
    "            length = len(text)\n",
    "            text += html\n",
    "            if len(text) <= length:\n",
    "                print \"Text appending isn't working as expected\"\n",
    "            else :\n",
    "                print \"Text appending may have worked\"\n",
    "            if downloadCount == 1:\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "            if downloadCount %5 == 0:\n",
    "                print '[+] Page: ' + str(downloadCount) + ' has been downloaded!'\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "        except:\n",
    "            downloadError = 1\n",
    "\n",
    "    stopTime = time.time()\n",
    "    runningTime = (stopTime - startTime)/60;\n",
    "    print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes' \t\t\n",
    "    return BeautifulSoup(text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_shares(url):\n",
    "    '''\n",
    "    This function reads the url using the phanton browser, phantom because it won't open a window,\n",
    "    read the webpage and find the number of shares of the link.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    driver = webdriver.PhantomJS('/media/sanhita/Windows7_OS/workspace/downloads/phantomjs-2.1.1-linux-x86_64/bin/phantomjs')\n",
    "    driver.get(url) #navigate to the page using the code; it takes a while to run this call\n",
    "    time.sleep(5)\n",
    "    html_source = driver.page_source \n",
    "    driver.quit()\n",
    "    \n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "    for number in soup.find('div', {\"class\" : \"sharecount__value--short\"}):\n",
    "        n = number\n",
    "    if not number:            \n",
    "        number = 0\n",
    "    \n",
    "    return int(n)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls.distinct(key='url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To retrieve data from mongodb, using id as the identifier\n",
    "\n",
    "for url in urls.find({'id': 0}):\n",
    "    print url['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls.find({'id': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_shares' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4810cd5b75a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_shares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.theguardian.com/commentisfree/2016/jan/14/prevent-british-women-isis-syria-met-police'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'num_shares' is not defined"
     ]
    }
   ],
   "source": [
    "num_shares('https://www.theguardian.com/commentisfree/2016/jan/14/prevent-british-women-isis-syria-met-police')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/commentisfree/2017/aug/13/america-carpet-bombed-north-korea-remember-that-past\"\n",
    "\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "# print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/commentisfree/2017/oct/23/houston-harvey-construction-workers-protect-abuse-undocumented\"\n",
    "url = 'https://www.theguardian.com/commentisfree/2016/jan/14/prevent-british-women-isis-syria-met-police'\n",
    "soup = download_comments(url, \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments = soup.findAll(\"div\", { \"class\" : \"d-comment__body\" })\n",
    "recommends = soup.findAll(\"span\", {\"class\" : \"d-comment__recommend-count--old\"})\n",
    "users = soup.findAll(\"span\", {\"itemprop\" : \"givenName\"})\n",
    "replies = soup.findAll(\"span\", {\"class\" : \"d-comment__reply-to-author\"})\n",
    "links = soup.findAll(\"a\", {\"itemprop\" : \"d-comment__reply-to-author\"})\n",
    "\n",
    "i = 0\n",
    "for user, comment, recommend, reply in zip(users, comments, recommends, replies):\n",
    "    if i == 100:\n",
    "        print user.text, comment.text, int(recommend.text), reply.text\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_ids = soup.findAll(\"li\", attrs_=\"data-comment-author-id\"})\n",
    "comment_ids = soup.findAll(\"li\", {\"id\" : \"data-comment-id\"})\n",
    "authors = soup.findAll(\"li\", {\"id\" : \"data-comment-author\"})\n",
    "\n",
    "len(comment_ids), len(user_ids), len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "lst = []\n",
    "for lis in soup.find_all('li'):\n",
    "    if 'data-comment-author-id' in (lis.attrs) and 'data-comment-id' in (lis.attrs)and 'data-comment-author' in (lis.attrs):\n",
    "#         print lis.attrs['data-comment-author-id']\n",
    "        i += 1\n",
    "\n",
    "print  i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in xrange (len(lst)):\n",
    "    if 'data-comment-author-id' in lst[i]:\n",
    "        print i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int(recommend.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(replies), len(users), len(comments), len(recommends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup.find('li', attrs={'id': 'data-comment-author-id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"GuardianDownload_4fmtq_1508968524.96_test.html\", 'r')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = BeautifulSoup(text, 'html.parser')\n",
    "comments = s2.findAll(\"div\", { \"class\" : \"d-comment__body\" })\n",
    "recommends = s2.findAll(\"span\", {\"class\" : \"d-comment__recommend-count--old\"})\n",
    "users = s2.findAll(\"span\", {\"itemprop\" : \"givenName\"})\n",
    "replies = s2.findAll(\"span\", {\"class\" : \"d-comment__reply-to-author\"})\n",
    "\n",
    "len(replies), len(users), len(comments), len(recommends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/us/commentisfree\"\n",
    "\n",
    "list_of_urls = requests.get(url).text\n",
    "\n",
    "soup2 = BeautifulSoup(list_of_urls, \"html.parser\")\n",
    "\n",
    "\n",
    "import re\n",
    "i = 0\n",
    "for link in soup2.findAll('a', attrs={'href': re.compile(\"https://www.theguardian.com/commentisfree/2017\")}):\n",
    "    print link.get('href')\n",
    "    i += 1\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/commentisfree/2016/jan/14/prevent-british-women-isis-syria-met-police'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(urllib2.urlopen(url).read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "lst = []\n",
    "for lis in soup.find_all('a'):\n",
    "    if 'data-link-name' in (lis.attrs) and 'submeta__link' in lis.attrs:\n",
    "        print lis.attrs['data-link-name']\n",
    "        i += 1\n",
    "\n",
    "print  i\n",
    "\n",
    "\n",
    "# for topic in soup.find_all(\"a\", { \"class\" : \"data-link-name\" }):\n",
    "#     print topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topic in soup.findAll(\"a\", {\"class\" : \"submeta__link\"}):\n",
    "    if 'data-link-name' in topic.attrs and 'keyword: ' in topic.attrs['data-link-name']:\n",
    "        print topic.attrs['data-link-name'][9:]\n",
    "# print section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in soup.find(\"h1\", {\"class\" : \"content__headline\"}):\n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not run the cell below this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The one below is a github code for downloading comments. Do not use this; \n",
    "# instead use a modified version of this in the cell below this one.\n",
    "\n",
    "import urllib2\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# This script will download all comments from a Guardian (theguardian.com) article into a single .html file\n",
    "# Use 'File ID' to easily identify your file later (this could also be something like a Citavi identifier)\n",
    "# Ingo Kleiber <ikleiber@googlemail.com>\n",
    "# https://gist.github.com/IngoKl/3ebd968ef3c6da406638\n",
    "\n",
    "print \"\"\"\\\n",
    "   _____                     _ _             _____  _      \n",
    "  / ____|                   | (_)           |  __ \\| |     \n",
    " | |  __ _   _  __ _ _ __ __| |_  __ _ _ __ | |  | | |     \n",
    " | | |_ | | | |/ _` | '__/ _` | |/ _` | '_ \\| |  | | |     \n",
    " | |__| | |_| | (_| | | | (_| | | (_| | | | | |__| | |____ \n",
    "  \\_____|\\__,_|\\__,_|_|  \\__,_|_|\\__,_|_| |_|_____/|______|\n",
    "\"\"\"                                                         \n",
    "\n",
    "print 'Guardian Comment Downloader v.1.1 - Ingo Kleiber (05.01.2015)\\n'\n",
    "guardianLink = raw_input ('> Guadian Link: ')\n",
    "fileId = raw_input('> File ID: ')\n",
    "\n",
    "#Figuring out the PageId\n",
    "pageResponse = urllib2.urlopen(guardianLink)\n",
    "commentsMatch = re.search(r'/p/(.*?)\"', pageResponse.read(), re.M|re.I)\n",
    "# print \"Comments Match = \", commentsMatch()\n",
    "if commentsMatch:\n",
    "    pageId = commentsMatch.group(1)\n",
    "    print '[+] pageId has been retrieved ('+pageId+')'\n",
    "else:\n",
    "    sys.exit('[-] Could not retrieve pageId!')\n",
    "\n",
    "#Retrieving comments\n",
    "downloadCount = 1\n",
    "downloadError = 0\n",
    "\n",
    "startTime = time.time()\n",
    "fileName = 'GuardianDownload_' + pageId + '_' + str(startTime) + '_' + fileId + '.html'\n",
    "f = open(fileName, 'a')\n",
    "\n",
    "\n",
    "print '[~] Start downloading pages ...'\n",
    "while downloadError == 0:\n",
    "    try:\n",
    "        response = urllib2.urlopen('http://www.theguardian.com/discussion/p/' + pageId + '?page=' + str(downloadCount))\n",
    "        html = response.read()\n",
    "        f.write(html)\n",
    "        if downloadCount == 1:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "        print '[+] Page: ' + str(downloadCount) + ' has been downloaded!'\n",
    "        downloadCount = downloadCount + 1\n",
    "        \n",
    "    except:\n",
    "        downloadError = 1\n",
    "\n",
    "stopTime = time.time()\n",
    "runningTime = (stopTime - startTime)/60;\n",
    "print '[=] ' + str(downloadCount-1) + ' pages have been downloaded in ' + str(round(runningTime,2)) + ' minutes' \t\t\n",
    "f.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
